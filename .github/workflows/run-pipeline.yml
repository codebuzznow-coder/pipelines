# GitHub Actions workflow to run data pipeline
# This can be triggered manually or when data files are uploaded

name: Run Data Pipeline

on:
  workflow_dispatch:
    inputs:
      s3_path:
        description: 'S3 URI only (e.g. s3://survey-qa-data-301625833185/survey_data/) — NOT the console URL'
        required: true
        default: ''
      sample_pct:
        description: 'Sample percentage (1-100)'
        required: false
        default: '5'
      seed:
        description: 'Random seed'
        required: false
        default: '42'

env:
  AWS_REGION: us-east-1

jobs:
  run-pipeline:
    name: Run Data Pipeline on EC2
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Validate S3 path
        run: |
          S3_PATH="${{ github.event.inputs.s3_path }}"
          if [[ ! "$S3_PATH" =~ ^s3:// ]]; then
            echo "::error::S3_PATH must be an S3 URI, e.g. s3://survey-qa-data-301625833185/survey_data/"
            echo "You entered something like a console URL. Use the bucket name and prefix only."
            exit 1
          fi
          echo "Using S3 path: $S3_PATH"
      
      - name: Run Data Pipeline (via SSM)
        if: secrets.EC2_INSTANCE_ID != ''
        env:
          EC2_INSTANCE_ID: ${{ secrets.EC2_INSTANCE_ID }}
          S3_PATH: ${{ github.event.inputs.s3_path }}
          SAMPLE_PCT: ${{ github.event.inputs.sample_pct }}
          SEED: ${{ github.event.inputs.seed }}
          REGION: ${{ env.AWS_REGION }}
        run: |
          set -e
          echo "Using AWS Systems Manager (no SSH required)..."
          
          # Build script (S3_PATH, REGION, SAMPLE_PCT, SEED expanded here on runner)
          SCRIPT="set -e
          echo '=== Running Data Pipeline ==='
          echo 'S3 Path: $S3_PATH'
          TEMP_DIR=\$(mktemp -d)
          echo 'Downloading files from S3...'
          aws s3 sync '$S3_PATH' \"\$TEMP_DIR/\" --region '$REGION' --exclude '*' --include '*.csv' --include '*.zip'
          CSV_COUNT=\$(find \"\$TEMP_DIR\" -name '*.csv' 2>/dev/null | wc -l)
          ZIP_COUNT=\$(find \"\$TEMP_DIR\" -name '*.zip' 2>/dev/null | wc -l)
          if [ \"\$CSV_COUNT\" -eq 0 ] && [ \"\$ZIP_COUNT\" -eq 0 ]; then echo 'ERROR: No CSV or ZIP files found'; exit 1; fi
          echo \"Found \$CSV_COUNT CSV file(s), \$ZIP_COUNT ZIP file(s)\"
          echo 'Running pipeline...'
          sudo docker exec survey-qa python -m data_pipeline.run_pipeline --input \"\$TEMP_DIR\" --sample-pct $SAMPLE_PCT --seed $SEED
          rm -rf \"\$TEMP_DIR\"
          echo '=== Pipeline completed successfully ==='"
          
          CMD_JSON=$(jq -n --arg cmd "$SCRIPT" '{"commands":[$cmd]}')
          
          COMMAND_ID=$(aws ssm send-command \
            --instance-ids "$EC2_INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --parameters "$CMD_JSON" \
            --region "$REGION" \
            --output text \
            --query "Command.CommandId")
          
          echo "Waiting for command $COMMAND_ID on instance $EC2_INSTANCE_ID..."
          for i in {1..60}; do
            STATUS=$(aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "$EC2_INSTANCE_ID" --region "$REGION" --query "Status" --output text 2>/dev/null || echo "Pending")
            [ "$STATUS" = "Success" ] && break
            [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Cancelled" ] && break
            sleep 5
          done
          
          OUT=$(aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "$EC2_INSTANCE_ID" --region "$REGION" --output json)
          echo "--- SSM stdout ---"
          echo "$OUT" | jq -r '.StandardOutputContent // empty'
          echo "--- SSM stderr ---"
          echo "$OUT" | jq -r '.StandardErrorContent // empty'
          
          STATUS=$(echo "$OUT" | jq -r '.Status')
          if [ "$STATUS" != "Success" ]; then
            echo "::error::SSM command failed with status: $STATUS"
            exit 1
          fi
      
      - name: Run Data Pipeline (via SSH)
        if: secrets.EC2_INSTANCE_ID == '' && secrets.EC2_HOST != ''
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_KEY }}
          S3_PATH: ${{ github.event.inputs.s3_path }}
          SAMPLE_PCT: ${{ github.event.inputs.sample_pct }}
          SEED: ${{ github.event.inputs.seed }}
          REGION: ${{ env.AWS_REGION }}
        run: |
          mkdir -p ~/.ssh
          echo "$EC2_SSH_KEY" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -i ~/.ssh/deploy_key ec2-user@$EC2_HOST "set -e
            echo '=== Running Data Pipeline ==='
            TEMP_DIR=\$(mktemp -d)
            aws s3 sync '$S3_PATH' \"\$TEMP_DIR/\" --region $REGION --exclude '*' --include '*.csv' --include '*.zip'
            CSV_COUNT=\$(find \"\$TEMP_DIR\" -name '*.csv' 2>/dev/null | wc -l)
            ZIP_COUNT=\$(find \"\$TEMP_DIR\" -name '*.zip' 2>/dev/null | wc -l)
            if [ \"\$CSV_COUNT\" -eq 0 ] && [ \"\$ZIP_COUNT\" -eq 0 ]; then echo 'ERROR: No CSV or ZIP files found'; exit 1; fi
            sudo docker exec survey-qa python -m data_pipeline.run_pipeline --input \"\$TEMP_DIR\" --sample-pct $SAMPLE_PCT --seed $SEED
            rm -rf \"\$TEMP_DIR\"
            echo '=== Pipeline completed successfully ==='"
      
      - name: Require EC2_INSTANCE_ID or EC2_HOST
        if: secrets.EC2_INSTANCE_ID == '' && secrets.EC2_HOST == ''
        run: |
          echo "::error::Add EC2_INSTANCE_ID (recommended) or EC2_HOST to GitHub secrets. SSM works without opening SSH to the internet."
          exit 1
      
      - name: Pipeline Status
        if: success()
        run: |
          echo "✅ Data pipeline completed successfully!"
          if [ -n "${{ secrets.EC2_HOST }}" ]; then
            echo "View results at: http://${{ secrets.EC2_HOST }}:8501"
          fi
