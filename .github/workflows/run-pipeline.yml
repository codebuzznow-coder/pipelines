# GitHub Actions workflow to run data pipeline
# This can be triggered manually or when data files are uploaded

name: Run Data Pipeline

on:
  workflow_dispatch:
    inputs:
      s3_path:
        description: 'S3 URI only (e.g. s3://survey-qa-data-301625833185/survey_data/) — NOT the console URL'
        required: true
        default: ''
      sample_pct:
        description: 'Sample percentage (1-100)'
        required: false
        default: '5'
      seed:
        description: 'Random seed'
        required: false
        default: '42'

env:
  AWS_REGION: us-east-1

jobs:
  run-pipeline:
    name: Run Data Pipeline on EC2
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Validate S3 path
        run: |
          S3_PATH="${{ github.event.inputs.s3_path }}"
          if [[ ! "$S3_PATH" =~ ^s3:// ]]; then
            echo "::error::S3_PATH must be an S3 URI, e.g. s3://survey-qa-data-301625833185/survey_data/"
            echo "You entered something like a console URL. Use the bucket name and prefix only."
            exit 1
          fi
          echo "Using S3 path: $S3_PATH"
      
      - name: Run Data Pipeline
        env:
          EC2_INSTANCE_ID: ${{ secrets.EC2_INSTANCE_ID }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_SSH_KEY: ${{ secrets.EC2_SSH_KEY }}
          S3_PATH: ${{ github.event.inputs.s3_path }}
          SAMPLE_PCT: ${{ github.event.inputs.sample_pct }}
          SEED: ${{ github.event.inputs.seed }}
          REGION: ${{ env.AWS_REGION }}
        run: |
          set -e
          if [ -z "$EC2_INSTANCE_ID" ] && [ -z "$EC2_HOST" ]; then
            echo "::error::Add EC2_INSTANCE_ID or EC2_HOST to repo secrets (Settings -> Secrets -> Actions)."
            exit 1
          fi

          if [ -n "$EC2_INSTANCE_ID" ]; then
            echo "Using AWS Systems Manager..."
            SCRIPT="set -e; echo '=== Running Data Pipeline ==='; TEMP_DIR=\$(mktemp -d); aws s3 sync '$S3_PATH' \$TEMP_DIR/ --region '$REGION' --exclude '*' --include '*.csv' --include '*.zip'; CSV_COUNT=\$(find \$TEMP_DIR -name '*.csv' 2>/dev/null | wc -l); ZIP_COUNT=\$(find \$TEMP_DIR -name '*.zip' 2>/dev/null | wc -l); if [ \$CSV_COUNT -eq 0 ] && [ \$ZIP_COUNT -eq 0 ]; then echo 'ERROR: No CSV or ZIP in S3'; exit 1; fi; sudo docker exec survey-qa python -m data_pipeline.run_pipeline --input \$TEMP_DIR --sample-pct $SAMPLE_PCT --seed $SEED; rm -rf \$TEMP_DIR; echo 'Done'"
            CMD_JSON=$(jq -n --arg cmd "$SCRIPT" '{"commands":[$cmd]}')
            COMMAND_ID=$(aws ssm send-command --instance-ids "$EC2_INSTANCE_ID" --document-name "AWS-RunShellScript" --parameters "$CMD_JSON" --region "$REGION" --output text --query "Command.CommandId")
            echo "Command ID: $COMMAND_ID (waiting up to 5 min)..."
            for i in $(seq 1 60); do
              STATUS=$(aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "$EC2_INSTANCE_ID" --region "$REGION" --query "Status" --output text 2>/dev/null || echo "Pending")
              echo "  Status: $STATUS"
              [ "$STATUS" = "Success" ] && break
              [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Cancelled" ] && break
              sleep 5
            done
            OUT=$(aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "$EC2_INSTANCE_ID" --region "$REGION" --output json)
            echo "--- stdout ---"
            echo "$OUT" | jq -r '.StandardOutputContent // empty'
            echo "--- stderr ---"
            echo "$OUT" | jq -r '.StandardErrorContent // empty'
            STATUS=$(echo "$OUT" | jq -r '.Status')
            if [ "$STATUS" != "Success" ]; then
              echo "::error::SSM command status: $STATUS"
              exit 1
            fi
          else
            echo "Using SSH..."
            [ -z "$EC2_SSH_KEY" ] && { echo "::error::EC2_SSH_KEY secret required when using EC2_HOST."; exit 1; }
            mkdir -p ~/.ssh
            echo "$EC2_SSH_KEY" > ~/.ssh/deploy_key
            chmod 600 ~/.ssh/deploy_key
            REMOTE_CMD="TEMP_DIR=\$(mktemp -d); aws s3 sync '$S3_PATH' \$TEMP_DIR/ --region '$REGION' --exclude '*' --include '*.csv' --include '*.zip'; sudo docker exec survey-qa python -m data_pipeline.run_pipeline --input \$TEMP_DIR --sample-pct $SAMPLE_PCT --seed $SEED; rm -rf \$TEMP_DIR"
            ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 -i ~/.ssh/deploy_key ec2-user@$EC2_HOST "$REMOTE_CMD"
          fi
          echo "✅ Pipeline completed."
          [ -n "$EC2_HOST" ] && echo "App: http://$EC2_HOST:8501"
